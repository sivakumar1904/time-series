{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OD3jPG8PvG1x"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "forecast_project.py\n",
        "\n",
        "Comprehensive project script for:\n",
        "- Generating / Loading multivariate time series data\n",
        "- Preprocessing: missing values, scaling, lag features\n",
        "- Models: ARIMA baseline (univariate), PyTorch LSTM, Attention-LSTM, Transformer\n",
        "- Expanding-window cross-validation for hyperparameter tuning\n",
        "- Evaluation: MAE, RMSE, MAPE\n",
        "- Attention interpretation utilities\n",
        "\n",
        "NOTE: This script is written to be understandable and modifiable. For heavy training\n",
        "you should run on a machine with GPU and adjust hyperparameters accordingly.\n",
        "\n",
        "Dependencies:\n",
        "- numpy, pandas, scikit-learn, torch, matplotlib, tqdm\n",
        "- statsmodels (for ARIMA baseline) [optional but recommended]\n",
        "- joblib (optional for saving objects)\n",
        "\n",
        "Run example:\n",
        "python forecast_project.py --data /path/to/synthetic_multivariate_timeseries.csv --target sensor_1\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import json\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Optional import for ARIMA baseline\n",
        "try:\n",
        "    import statsmodels.api as sm\n",
        "    from statsmodels.tsa.arima.model import ARIMA\n",
        "    STATSMODELS_AVAILABLE = True\n",
        "except Exception:\n",
        "    STATSMODELS_AVAILABLE = False\n",
        "\n",
        "# ------------------------\n",
        "# Utilities\n",
        "# ------------------------\n",
        "def mae(y_true, y_pred):\n",
        "    return mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return math.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    mask = y_true != 0\n",
        "    if mask.sum() == 0:\n",
        "        return float(\"nan\")\n",
        "    return (np.abs((y_true - y_pred)[mask] / y_true[mask])).mean() * 100\n",
        "\n",
        "# ------------------------\n",
        "# Data pipeline\n",
        "# ------------------------\n",
        "def load_and_preprocess(path, target_col, lags=[1,24,168], test_size=0.2, val_size=0.1):\n",
        "    \"\"\"\n",
        "    Loads CSV with a timestamp column named 'timestamp' and numeric feature columns.\n",
        "    Performs:\n",
        "    - parse dates\n",
        "    - forward-fill/backfill for missing values\n",
        "    - scaling (StandardScaler)\n",
        "    - creates lag features for all numeric columns\n",
        "    - splits into train/val/test by time\n",
        "    Returns dict with scalers and arrays.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path, parse_dates=[\"timestamp\"])\n",
        "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "    # identify numeric columns (excluding timestamp)\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if target_col not in numeric_cols:\n",
        "        raise ValueError(f\"target_col '{target_col}' not numeric or not present.\")\n",
        "    # missing value handling\n",
        "    df[numeric_cols] = df[numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
        "    # create lag features\n",
        "    for lag in lags:\n",
        "        for c in numeric_cols:\n",
        "            df[f\"{c}_lag_{lag}\"] = df[c].shift(lag)\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "    # feature columns\n",
        "    lag_cols = [c for c in df.columns if \"_lag_\" in c]\n",
        "    # Additional time features\n",
        "    df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
        "    df[\"dayofweek\"] = df[\"timestamp\"].dt.dayofweek\n",
        "    # Choose features (include some external covariates if present)\n",
        "    feature_cols = lag_cols.copy()\n",
        "    # keep temperature / holiday if present\n",
        "    if \"temperature\" in df.columns:\n",
        "        feature_cols.append(\"temperature\")\n",
        "    if \"is_holiday\" in df.columns:\n",
        "        feature_cols.append(\"is_holiday\")\n",
        "    feature_cols += [\"hour\", \"dayofweek\"]\n",
        "    # scaling\n",
        "    scaler = StandardScaler()\n",
        "    df_features = scaler.fit_transform(df[feature_cols])\n",
        "    X = df_features\n",
        "    y = df[target_col].values.reshape(-1,1)\n",
        "    # train/val/test split by time (chronological)\n",
        "    n = len(df)\n",
        "    test_n = int(n * test_size)\n",
        "    val_n = int(n * val_size)\n",
        "    train_end = n - test_n - val_n\n",
        "    X_train = X[:train_end]\n",
        "    y_train = y[:train_end]\n",
        "    X_val = X[train_end:train_end+val_n]\n",
        "    y_val = y[train_end:train_end+val_n]\n",
        "    X_test = X[train_end+val_n:]\n",
        "    y_test = y[train_end+val_n:]\n",
        "    return {\n",
        "        \"X_train\": X_train, \"y_train\": y_train,\n",
        "        \"X_val\": X_val, \"y_val\": y_val,\n",
        "        \"X_test\": X_test, \"y_test\": y_test,\n",
        "        \"scaler\": scaler, \"feature_cols\": feature_cols,\n",
        "        \"df_processed\": df  # returning for ARIMA indexing convenience\n",
        "    }\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# ------------------------\n",
        "# Models\n",
        "# ------------------------\n",
        "class LSTMForecaster(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        # We'll treat input as a single time-step sequence with many features (feature-engineered)\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "        self.hidden_dim = hidden_dim\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, features) -> reshape to (batch, seq_len=1, features)\n",
        "        x = x.unsqueeze(1)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.W = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "        self.u = nn.Linear(hidden_dim, 1, bias=False)\n",
        "    def forward(self, H):\n",
        "        # H: (batch, seq_len, hidden_dim)\n",
        "        score = torch.tanh(self.W(H))  # (batch, seq_len, hidden_dim)\n",
        "        score = self.u(score).squeeze(-1)  # (batch, seq_len)\n",
        "        alpha = torch.softmax(score, dim=1).unsqueeze(-1)  # (batch, seq_len, 1)\n",
        "        context = (H * alpha).sum(dim=1)  # (batch, hidden_dim)\n",
        "        return context, alpha\n",
        "\n",
        "class AttentionLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=False)\n",
        "        self.att = AttentionLayer(hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # seq_len=1\n",
        "        H, _ = self.lstm(x)  # (batch, seq_len, hidden_dim)\n",
        "        context, alpha = self.att(H)\n",
        "        out = self.fc(context)\n",
        "        return out, alpha\n",
        "\n",
        "class SimpleTransformerForecaster(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_encoder_layers=2, dim_feedforward=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        self.fc = nn.Linear(d_model, 1)\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # seq_len=1\n",
        "        x = self.input_proj(x)\n",
        "        h = self.transformer(x)\n",
        "        out = self.fc(h[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# ------------------------\n",
        "# Training / Evaluation\n",
        "# ------------------------\n",
        "def train_model(model, optimizer, criterion, train_loader, val_loader=None, epochs=10, device='cpu'):\n",
        "    model.to(device)\n",
        "    best_val_loss = float('inf')\n",
        "    best_state = None\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(xb)\n",
        "            # model may return (out, alpha)\n",
        "            if isinstance(out, tuple):\n",
        "                pred = out[0]\n",
        "            else:\n",
        "                pred = out\n",
        "            loss = criterion(pred, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "        avg_train = np.mean(train_losses) if train_losses else float('nan')\n",
        "        if val_loader is not None:\n",
        "            model.eval()\n",
        "            val_losses = []\n",
        "            with torch.no_grad():\n",
        "                for xb, yb in val_loader:\n",
        "                    xb, yb = xb.to(device), yb.to(device)\n",
        "                    out = model(xb)\n",
        "                    pred = out[0] if isinstance(out, tuple) else out\n",
        "                    loss = criterion(pred, yb)\n",
        "                    val_losses.append(loss.item())\n",
        "            avg_val = np.mean(val_losses) if val_losses else float('nan')\n",
        "            if avg_val < best_val_loss:\n",
        "                best_val_loss = avg_val\n",
        "                best_state = {k:v.cpu().clone() for k,v in model.state_dict().items()}\n",
        "        else:\n",
        "            avg_val = None\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - train_loss: {avg_train:.6f} - val_loss: {avg_val}\")\n",
        "    if best_state:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X, y, batch_size=128, device='cpu'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    ds = TimeSeriesDataset(X, y)\n",
        "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
        "    preds = []\n",
        "    trues = []\n",
        "    alphas = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            out = model(xb)\n",
        "            if isinstance(out, tuple):\n",
        "                pred, alpha = out\n",
        "                alphas.append(alpha.cpu().numpy())\n",
        "            else:\n",
        "                pred = out\n",
        "            preds.append(pred.cpu().numpy())\n",
        "            trues.append(yb.numpy())\n",
        "    y_pred = np.vstack(preds).ravel()\n",
        "    y_true = np.vstack(trues).ravel()\n",
        "    return y_true, y_pred, alphas\n",
        "\n",
        "# ------------------------\n",
        "# ARIMA baseline\n",
        "# ------------------------\n",
        "def arima_baseline(series, train_end_index, forecast_horizon):\n",
        "    \"\"\"\n",
        "    Trains a simple ARIMA on the training portion of `series` (1-D numpy array) and forecasts next `forecast_horizon` points.\n",
        "    Returns series_pred (for the test period).\n",
        "    Requires statsmodels.\n",
        "    \"\"\"\n",
        "    if not STATSMODELS_AVAILABLE:\n",
        "        raise RuntimeError(\"statsmodels not installed. Install statsmodels to run ARIMA baseline.\")\n",
        "    train = series[:train_end_index]\n",
        "    model = ARIMA(train, order=(5,1,0))\n",
        "    res = model.fit()\n",
        "    preds = res.forecast(steps=forecast_horizon)\n",
        "    return preds\n",
        "\n",
        "# ------------------------\n",
        "# Cross-validation (expanding window)\n",
        "# ------------------------\n",
        "def expanding_window_cv(X, y, model_builder, param_dict, n_splits=3, initial_train_size=None, val_size=500, device='cpu'):\n",
        "    \"\"\"\n",
        "    Simple expanding-window cross-validation to tune hyperparameters in `param_dict` (a dict).\n",
        "    model_builder is a function that accepts **params and returns an uninitialized PyTorch model.\n",
        "    Returns best_params and history.\n",
        "    \"\"\"\n",
        "    from itertools import product\n",
        "    keys = list(param_dict.keys())\n",
        "    grid = [dict(zip(keys, vals)) for vals in product(*param_dict.values())]\n",
        "    best = None\n",
        "    best_score = float('inf')\n",
        "    history = []\n",
        "    n = len(X)\n",
        "    if initial_train_size is None:\n",
        "        initial_train_size = int(n * 0.5)\n",
        "    for params in grid:\n",
        "        scores = []\n",
        "        for split in range(n_splits):\n",
        "            train_end = initial_train_size + split * val_size\n",
        "            val_start = train_end\n",
        "            val_end = train_end + val_size\n",
        "            if val_end > n:\n",
        "                break\n",
        "            X_train = X[:train_end]\n",
        "            y_train = y[:train_end]\n",
        "            X_val = X[val_start:val_end]\n",
        "            y_val = y[val_start:val_end]\n",
        "            # build model\n",
        "            model = model_builder(**params)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=params.get(\"lr\", 1e-3), weight_decay=params.get(\"weight_decay\", 0.0))\n",
        "            criterion = nn.MSELoss()\n",
        "            train_ds = TimeSeriesDataset(X_train, y_train)\n",
        "            val_ds = TimeSeriesDataset(X_val, y_val)\n",
        "            train_loader = DataLoader(train_ds, batch_size=params.get(\"batch_size\", 256), shuffle=True)\n",
        "            val_loader = DataLoader(val_ds, batch_size=params.get(\"batch_size\", 256), shuffle=False)\n",
        "            model = train_model(model, optimizer, criterion, train_loader, val_loader=val_loader, epochs=params.get(\"epochs\", 5), device=device)\n",
        "            y_val_true, y_val_pred, _ = evaluate_model(model, X_val, y_val, batch_size=256, device=device)\n",
        "            score = rmse(y_val_true, y_val_pred)\n",
        "            scores.append(score)\n",
        "        avg_score = np.mean(scores) if scores else float('inf')\n",
        "        history.append((params, avg_score))\n",
        "        if avg_score < best_score:\n",
        "            best_score = avg_score\n",
        "            best = params\n",
        "    return best, history\n",
        "\n",
        "# ------------------------\n",
        "# Main runner\n",
        "# ------------------------\n",
        "def main(args):\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    data = load_and_preprocess(args.data, target_col=args.target, lags=[1,24,168], test_size=0.2, val_size=0.1)\n",
        "    X_train, y_train = data[\"X_train\"], data[\"y_train\"]\n",
        "    X_val, y_val = data[\"X_val\"], data[\"y_val\"]\n",
        "    X_test, y_test = data[\"X_test\"], data[\"y_test\"]\n",
        "    input_dim = X_train.shape[1]\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Device:\", device)\n",
        "    # Baseline ARIMA (univariate) - using the original series (not scaled) for interpretability\n",
        "    print(\"Running ARIMA baseline (univariate) ...\")\n",
        "    if STATSMODELS_AVAILABLE:\n",
        "        # Need original series without lagging and scaling; reload raw series from CSV\n",
        "        df = pd.read_csv(args.data, parse_dates=[\"timestamp\"])\n",
        "        df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "        # For simplicity, we'll use last len(X_test) points as test horizon\n",
        "        total_len = len(df)\n",
        "        horizon = len(X_test)\n",
        "        try:\n",
        "            arima_preds = arima_baseline(df[args.target].values, total_len - horizon, horizon)\n",
        "            arima_rmse = rmse(df[args.target].values[total_len - horizon:], arima_preds)\n",
        "            print(f\"ARIMA RMSE: {arima_rmse:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(\"ARIMA training failed:\", e)\n",
        "            arima_preds = None\n",
        "    else:\n",
        "        print(\"statsmodels not available; skipping ARIMA baseline. Install statsmodels to enable this baseline.\")\n",
        "        arima_preds = None\n",
        "\n",
        "    # Standard LSTM\n",
        "    print(\"Training standard LSTM ...\")\n",
        "    lstm_params = {\"input_dim\": input_dim, \"hidden_dim\": 64, \"num_layers\": 2, \"dropout\": 0.2}\n",
        "    lstm = LSTMForecaster(**lstm_params)\n",
        "    optimizer = torch.optim.Adam(lstm.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "    criterion = nn.MSELoss()\n",
        "    train_loader = DataLoader(TimeSeriesDataset(X_train, y_train), batch_size=256, shuffle=True)\n",
        "    val_loader = DataLoader(TimeSeriesDataset(X_val, y_val), batch_size=256, shuffle=False)\n",
        "    lstm = train_model(lstm, optimizer, criterion, train_loader, val_loader=val_loader, epochs=10, device=device)\n",
        "    y_true_lstm, y_pred_lstm, _ = evaluate_model(lstm, X_test, y_test, batch_size=256, device=device)\n",
        "    print(\"LSTM metrics - RMSE: %.4f MAE: %.4f MAPE: %.4f\" % (rmse(y_true_lstm, y_pred_lstm), mae(y_true_lstm, y_pred_lstm), mape(y_true_lstm, y_pred_lstm)))\n",
        "\n",
        "    # Attention LSTM\n",
        "    print(\"Training Attention-LSTM ...\")\n",
        "    att_params = {\"input_dim\": input_dim, \"hidden_dim\": 64, \"num_layers\": 1, \"dropout\": 0.1}\n",
        "    att_model = AttentionLSTM(**att_params)\n",
        "    optimizer = torch.optim.Adam(att_model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
        "    att_model = train_model(att_model, optimizer, criterion, train_loader, val_loader=val_loader, epochs=10, device=device)\n",
        "    y_true_att, y_pred_att, alphas = evaluate_model(att_model, X_test, y_test, batch_size=256, device=device)\n",
        "    print(\"Attention LSTM metrics - RMSE: %.4f MAE: %.4f MAPE: %.4f\" % (rmse(y_true_att, y_pred_att), mae(y_true_att, y_pred_att), mape(y_true_att, y_pred_att)))\n",
        "\n",
        "    # Transformer\n",
        "    print(\"Training Transformer ...\")\n",
        "    trans_params = {\"input_dim\": input_dim, \"d_model\": 64, \"nhead\": 4, \"num_encoder_layers\": 2, \"dim_feedforward\": 128}\n",
        "    trans_model = SimpleTransformerForecaster(**trans_params)\n",
        "    optimizer = torch.optim.Adam(trans_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "    trans_model = train_model(trans_model, optimizer, criterion, train_loader, val_loader=val_loader, epochs=10, device=device)\n",
        "    y_true_tr, y_pred_tr, _ = evaluate_model(trans_model, X_test, y_test, batch_size=256, device=device)\n",
        "    print(\"Transformer metrics - RMSE: %.4f MAE: %.4f MAPE: %.4f\" % (rmse(y_true_tr, y_pred_tr), mae(y_true_tr, y_pred_tr), mape(y_true_tr, y_pred_tr)))\n",
        "\n",
        "    # Summarize metrics\n",
        "    results = {\n",
        "        \"ARIMA\": {\n",
        "            \"preds\": None if arima_preds is None else arima_preds.tolist(),\n",
        "            \"rmse\": None if arima_preds is None else float(rmse(df[args.target].values[total_len - horizon:], arima_preds)),\n",
        "            \"mae\": None if arima_preds is None else float(mae(df[args.target].values[total_len - horizon:], arima_preds)),\n",
        "            \"mape\": None if arima_preds is None else float(mape(df[args.target].values[total_len - horizon:], arima_preds))\n",
        "        },\n",
        "        \"LSTM\": {\n",
        "            \"rmse\": float(rmse(y_true_lstm, y_pred_lstm)),\n",
        "            \"mae\": float(mae(y_true_lstm, y_pred_lstm)),\n",
        "            \"mape\": float(mape(y_true_lstm, y_pred_lstm))\n",
        "        },\n",
        "        \"AttentionLSTM\": {\n",
        "            \"rmse\": float(rmse(y_true_att, y_pred_att)),\n",
        "            \"mae\": float(mae(y_true_att, y_pred_att)),\n",
        "            \"mape\": float(mape(y_true_att, y_pred_att))\n",
        "        },\n",
        "        \"Transformer\": {\n",
        "            \"rmse\": float(rmse(y_true_tr, y_pred_tr)),\n",
        "            \"mae\": float(mae(y_true_tr, y_pred_tr)),\n",
        "            \"mape\": float(mape(y_true_tr, y_pred_tr))\n",
        "        }\n",
        "    }\n",
        "    out_path = args.output or \".\"\n",
        "    os.makedirs(out_path, exist_ok=True)\n",
        "    with open(os.path.join(out_path, \"results_summary.json\"), \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(\"Results saved to\", os.path.join(out_path, \"results_summary.json\"))\n",
        "\n",
        "    # Attention interpretation: if `alphas` collected, average them\n",
        "    if alphas:\n",
        "        # alphas is a list of arrays per batch; concatenate and average across samples\n",
        "        all_alphas = np.vstack([a.squeeze(1) for a in alphas])\n",
        "        mean_alpha = np.mean(all_alphas, axis=0)  # shape (seq_len,)\n",
        "        # save\n",
        "        np.savetxt(os.path.join(out_path, \"attention_mean.txt\"), mean_alpha)\n",
        "        print(\"Saved mean attention weights to attention_mean.txt\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, default=\"synthetic_multivariate_timeseries.csv\", help=\"Path to CSV file\")\n",
        "    parser.add_argument(\"--target\", type=str, default=\"sensor_1\", help=\"Target column name\")\n",
        "    parser.add_argument(\"--output\", type=str, default=\"./output\", help=\"Output directory\")\n",
        "    args = parser.parse_args()\n",
        "    main(args)\n"
      ]
    }
  ]
}